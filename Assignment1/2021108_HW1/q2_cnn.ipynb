{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'black_bear': 0, 'people': 1, 'birds': 2, 'dog': 3, 'brown_bear': 4, 'roe_deer': 5, 'wild_boar': 6, 'amur_tiger': 7, 'amur_leopard': 8, 'sika_deer': 9}\n",
    "target_size = (224,224)\n",
    "resize = transforms.Resize(target_size)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RussianWildlifeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.data_list = []\n",
    "        for label in os.listdir(self.img_dir):\n",
    "            for file in os.listdir(os.path.join(self.img_dir,label)):\n",
    "                self.data_list.append((file,label))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_name, label = self.data_list[idx]\n",
    "        img_path = os.path.join(self.img_dir, label, file_name)\n",
    "\n",
    "        img = torch.from_numpy(cv2.imread(img_path))\n",
    "        img = img.permute(2, 0, 1)\n",
    "        img = resize(img)\n",
    "        img = img/255\n",
    "        img = normalize(img)\n",
    "        return img.float(), label_map[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RussianWildlifeDataset('data')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(data))\n",
    "val_size = int(0.1 * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=8)\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = list(label_map.keys())\n",
    "\n",
    "train_counts = [0]*10\n",
    "val_counts = [0]*10\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    _, label = train_set[i]\n",
    "    train_counts[label] += 1\n",
    "\n",
    "for i in range(len(val_set)):\n",
    "    _, label = val_set[i]\n",
    "    val_counts[label] += 1\n",
    "\n",
    "# Plot bar charts\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.bar(class_labels, train_counts, label='Train')\n",
    "plt.bar(class_labels, val_counts, label='Validation')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Data Distribution in Train and Validation Sets')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"cv_ass1\", \n",
    "    name=f\"Q2_CNN_normalized\", \n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"Russian Wildlife Dataset\",\n",
    "    \"epochs\": 10,\n",
    "    })\n",
    "\n",
    "config = wandb.config   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_classes):\n",
    "\n",
    "    super(CNN, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "    )\n",
    "\n",
    "    self.conv2 = nn.Sequential(\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "    self.conv3 = nn.Sequential(\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(25088, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CNN(10) # Assuming 10 classes\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = config.epochs\n",
    "save_loss = 99999\n",
    "\n",
    "print(\"Started Training\")\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('Training for epoch: ', epoch)\n",
    "\n",
    "    model.train()\n",
    "    tloss = 0\n",
    "    tstep = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img, label = data\n",
    "        inputs = img.to(device)\n",
    "        labels = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.squeeze(1)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        train_loss =  criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        tstep+=1\n",
    "        tloss += train_loss.item()\n",
    "\n",
    "    tstep+=1\n",
    "    print('EPOCH:',epoch)\n",
    "    print('Average train loss:', tloss/tstep)\n",
    "\n",
    "    model.eval()\n",
    "    vloss = 0\n",
    "    vstep = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        img, label = data\n",
    "        inputs = img.to(device)\n",
    "        labels = label.to(device)\n",
    "\n",
    "        inputs = inputs.squeeze(1)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        val_loss =  criterion(outputs, labels)\n",
    "        vstep+=1\n",
    "        vloss += val_loss.item()\n",
    "\n",
    "    vstep+=1\n",
    "    print('EPOCH:',epoch)\n",
    "    print('Average val loss:', vloss/vstep)\n",
    "    if(vloss/vstep<save_loss):\n",
    "        save_loss = vloss/vstep\n",
    "        state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        torch.save(state, 'q2_cnn_best.pt')  \n",
    "\n",
    "    log_metric = {\"Epoch\":epoch, \"Train Loss\": tloss/tstep, \"Val Loss\": vloss/vstep}\n",
    "    wandb.log(log_metric)\n",
    "        \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, 'q2_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train loss keeps on reducing on training after each epoch which is expected, but after epoch 4. The val loss starts increasing which increase to 2.03. Thus the model is overtrained now\n",
    "Only keeping the train limited to 5 epochs would have been ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case best model is to be runned.\n",
    "# model = CNN(10)\n",
    "# state_dict = torch.load('q2_cnn_best.pt')\n",
    "# model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started Testing\")\n",
    "\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        img, labels = data\n",
    "        test_labels.extend(labels.numpy())\n",
    "\n",
    "        inputs = img.to(device)\n",
    "        inputs = inputs.squeeze(1)\n",
    "        \n",
    "        outputs = model(inputs).argmax(dim=1)\n",
    "        test_predictions.extend(outputs.numpy())\n",
    "        \n",
    "print(\"Finished Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "\n",
    "\n",
    "correct = (test_labels==test_predictions).sum()\n",
    "total = len(test_predictions)\n",
    "\n",
    "acc = correct/total\n",
    "f1score = f1_score(test_labels,test_predictions,average='macro')\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 Score: \", f1score)\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "wandb.log({\"Accuracy\":acc, \"F1 Score\":f1score})\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.show(plt)\n",
    "wandb.log({\"Confusion Matrix\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
